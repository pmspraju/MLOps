{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 09:29:24.106313: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-01 09:29:24.862484: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-01 09:29:24.862757: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-01 09:29:24.862766: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/pmspraju/.local/share/virtualenvs/imageResolution-c2hlZ-SW/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas:1.3.4, Numpy:1.21.4, Tensorflow:2.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfdata\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "from tensorflow.python.saved_model import signature_constants\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import *\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.exceptions import MlflowException\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, TensorSpec\n",
    "\n",
    "from prefect import flow, task\n",
    "from prefect.task_runners import SequentialTaskRunner\n",
    "\n",
    "print(f'Pandas:{pd.__version__}, Numpy:{np.__version__}, Tensorflow:{tf.__version__}')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "#print('Device:', tf.config.list_physical_devices('GPU'))\n",
    "#print(\"----Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlflow_setup():\n",
    "    MLFLOW_TRACKING_URI =\"sqlite:////home/pmspraju/tracking-server/mlflow.db\" \n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "    client = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)\n",
    "\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def create_mlflow_experiment(experiment_name):\n",
    "    try:\n",
    "        experiment_id = mlflow.create_experiment(\n",
    "            experiment_name,\n",
    "            #artifact_location=Path.cwd().joinpath(\"mlruns\").as_uri(),\n",
    "            artifact_location='//home/pmspraju/tracking-server/mlruns/',\n",
    "            tags={\"version\": \"v1\", \"priority\": \"P1\"},\n",
    "        )\n",
    "    except Exception as MlflowException:\n",
    "        print(f\"Experiment exists\")\n",
    "        experiment= mlflow.set_experiment(experiment_name)\n",
    "        # Examine the experiment details.\n",
    "        print(\"Experiment_id: {}\".format(experiment.experiment_id))\n",
    "        print(\"Name: {}\".format(experiment.name))\n",
    "        print(\"Artifact Location: {}\".format(experiment.artifact_location))\n",
    "        print(\"Tags: {}\".format(experiment.tags))\n",
    "        print(\"Lifecycle_stage: {}\".format(experiment.lifecycle_stage))\n",
    "        print(\"Last Updated timestamp: {}\".format(experiment.last_update_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the images in the dataset. We must also normalize the masks so that the classes are numbered from 0 through 2, \n",
    "# instead of from 1 through 3\n",
    "def normalize(input_image, input_mask):\n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "    input_mask -= 1\n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image, given an element from a TensorFlow dataset data structure. Note that we resize both the image and the mask to 256x256. \n",
    "# Also, if the train flag is set to True, we perform augmentation by randomly mirroring the image and its mask. \n",
    "# Finally, we normalize the inputs:\n",
    "@tf.function\n",
    "def load_image(dataset_element, train=True):\n",
    "\n",
    "    input_image = tf.image.resize(dataset_element['image'], (256, 256))\n",
    "    input_mask = tf.image.resize(dataset_element['segmentation_mask'],(256, 256))\n",
    "\n",
    "    if train and np.random.uniform() > 0.5:\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        input_mask = tf.image.flip_left_right(input_mask)\n",
    "\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "\n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_mlflow_image(name, remove_file=False):\n",
    "    work_dir = pathlib.Path('/home/pmspraju/MLOps/imageResolution')\n",
    "    img_path = os.path.join(work_dir, name)\n",
    "    im = Image.open(img_path)\n",
    "    mlflow.log_image(im, name)\n",
    "    \n",
    "    if remove_file:\n",
    "        os.remove(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class, UNet(), that will contain all the logic necessary to build, train, and evaluate our U-Net.\n",
    "class UNet(object):\n",
    "\n",
    "    # output_channels is, by default, 3, because each pixel can be categorized into one of three classes.\n",
    "    def __init__(self, input_size=(256, 256, 3), output_channels=3):\n",
    "        self.input_size = input_size\n",
    "        self.output_channels = output_channels\n",
    "        self.model = self._create_model()\n",
    "        loss = SparseCategoricalCrossentropy(from_logits=True)\n",
    "        self.model.compile(optimizer=RMSprop(), loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    # This is a convolution that can be (optionally) batch normalized and that's activated with LeakyReLU:\n",
    "    @staticmethod\n",
    "    def _downsample(filters, size, batch_norm=True):\n",
    "\n",
    "        initializer = tf.random_normal_initializer(0.0, 0.02)\n",
    "        layers = Sequential()\n",
    "\n",
    "        layers.add(Conv2D(filters=filters,\n",
    "                          kernel_size=size,\n",
    "                          strides=2,\n",
    "                          padding='same',\n",
    "                          kernel_initializer=initializer,\n",
    "                          use_bias=False))\n",
    "        \n",
    "        if batch_norm:\n",
    "            layers.add(BatchNormalization())\n",
    "\n",
    "        layers.add(LeakyReLU())\n",
    "\n",
    "        return layers\n",
    "    \n",
    "    # the _upsample() helper method expands its input through a transposed convolution, which is also batch normalized \n",
    "    # and ReLU activated (optionally, we can add a dropout layer to prevent overfitting):\n",
    "    def _upsample(self, filters, size, drop_out=False):\n",
    "\n",
    "        init = tf.random_normal_initializer(0.0, 0.02)\n",
    "\n",
    "        layers = Sequential()\n",
    "\n",
    "        layers.add(Conv2DTranspose(filters=filters,\n",
    "                                    kernel_size=size,\n",
    "                                    strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=init,\n",
    "                                    use_bias=False))\n",
    "        \n",
    "        layers.add(BatchNormalization())\n",
    "\n",
    "        if drop_out:\n",
    "            layers.add(Dropout(rate=0.5))\n",
    "\n",
    "        layers.add(ReLU())\n",
    "\n",
    "        return layers\n",
    "    \n",
    "    # The encoding part of the network is just a stack of downsampling blocks\n",
    "    # the decoding portion is, as expected, comprised of a series of upsampling blocks\n",
    "    def _create_model(self):\n",
    "\n",
    "        down_stack = [self._downsample(64, 4, batch_norm=False)]\n",
    "\n",
    "        for filters in (128, 256, 512, 512, 512, 512, 512):\n",
    "            down_block = self._downsample(filters, 4)\n",
    "            down_stack.append(down_block)\n",
    "\n",
    "        up_stack = []\n",
    "        for _ in range(3):\n",
    "            up_block = self._upsample(512, 4, drop_out=True)\n",
    "            up_stack.append(up_block)\n",
    "\n",
    "        for filters in (512, 256, 128, 64):\n",
    "            up_block = self._upsample(filters, 4)\n",
    "            up_stack.append(up_block)\n",
    "\n",
    "        inputs = Input(shape=self.input_size)\n",
    "\n",
    "        x = inputs\n",
    "\n",
    "        skip_layers = []\n",
    "\n",
    "        for down in down_stack:\n",
    "            x = down(x)\n",
    "            skip_layers.append(x)\n",
    "\n",
    "        skip_layers = reversed(skip_layers[:-1])\n",
    "\n",
    "        for up, skip_connection in zip(up_stack, skip_layers):\n",
    "            x = up(x)\n",
    "            x = Concatenate()([x, skip_connection])\n",
    "\n",
    "        init = tf.random_normal_initializer(0.0, 0.02)\n",
    "        output = Conv2DTranspose(   filters=self.output_channels,\n",
    "                                    kernel_size=3,\n",
    "                                    strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=init)(x)\n",
    "        \n",
    "        return Model(inputs, outputs=output)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _plot_model_history(model_history, metric, ylim=None):\n",
    "        \n",
    "        plt.style.use('seaborn-darkgrid')\n",
    "        plotter = tfdocs.plots.HistoryPlotter()\n",
    "        plotter.plot({'Model': model_history}, metric=metric)\n",
    "        plt.title(f'{metric.upper()}')\n",
    "\n",
    "        if ylim is None:\n",
    "            plt.ylim([0, 1])\n",
    "        else:\n",
    "            plt.ylim(ylim)\n",
    "\n",
    "        plt.savefig(f'{metric}.png')\n",
    "        plt.close()\n",
    "\n",
    "         # Log the image in mlflow\n",
    "        log_mlflow_image(f'{metric}.png', True)\n",
    "        \n",
    "\n",
    "    def train(self, train_dataset, epochs, steps_per_epoch, validation_dataset, validation_steps):\n",
    "        hist = self.model.fit(train_dataset,\n",
    "                                epochs=epochs,\n",
    "                                steps_per_epoch=steps_per_epoch,\n",
    "                                validation_steps=validation_steps,\n",
    "                                validation_data=validation_dataset)\n",
    "        \n",
    "        # write model summary\n",
    "        path = pathlib.Path('/mnt/c/Users/pmspr/Documents/Machine Learning/Courses/Tensorflow Cert/Data/dogscats')\n",
    "        summary = []\n",
    "        self.model.summary(print_fn=summary.append)\n",
    "        summary = \"\\n\".join(summary)\n",
    "        summary_path = os.path.join(path, \"model_summary.txt\")\n",
    "        with open(summary_path, \"w\") as f:\n",
    "            f.write(summary)\n",
    "        mlflow.log_artifact(summary_path)\n",
    "        os.remove(summary_path)\n",
    "\n",
    "        # write model as json file\n",
    "        model_json_path = os.path.join(path, \"model.json\")\n",
    "        with open(model_json_path, \"w\") as f:\n",
    "            f.write(self.model.to_json())\n",
    "        mlflow.log_artifact(model_json_path)\n",
    "        os.remove(model_json_path)\n",
    "        \n",
    "        # log model in mlflow\n",
    "        input_schema = Schema([\n",
    "                            TensorSpec(np.dtype(np.uint8), (-1, 256, 256, 3)),\n",
    "                            ])\n",
    "        output_schema = Schema([TensorSpec(np.dtype(np.uint8), (-1, 256, 256, 3))])\n",
    "        signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "\n",
    "        mlflow.tensorflow.log_model(model=self.model,signature=signature,\n",
    "                                   artifact_path=\"tf-models\")\n",
    "        \n",
    "        \n",
    "        self._plot_model_history(hist, 'loss', [0., 2.0])\n",
    "        self._plot_model_history(hist, 'accuracy')\n",
    "        \n",
    "    @staticmethod\n",
    "    def _process_mask(mask):\n",
    "        mask = (mask.numpy() * 127.5).astype('uint8')\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        return mask\n",
    "    \n",
    "    def _save_image_and_masks(self, image, ground_truth_mask, prediction_mask, image_id):\n",
    "\n",
    "        image = (image.numpy() * 255.0).astype('uint8')\n",
    "        gt_mask = self._process_mask(ground_truth_mask)\n",
    "        pred_mask = self._process_mask(prediction_mask)\n",
    "\n",
    "        mosaic = np.hstack([image, gt_mask, pred_mask])\n",
    "        mosaic = cv2.cvtColor(mosaic, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        cv2.imwrite(f'mosaic_{image_id}.jpg', mosaic)\n",
    "        log_mlflow_image(f'mosaic_{image_id}.jpg', True)\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_mask(prediction_mask):\n",
    "        prediction_mask = tf.argmax(prediction_mask, axis=-1)\n",
    "        prediction_mask = prediction_mask[...,tf.newaxis]\n",
    "        \n",
    "        return prediction_mask[0]\n",
    "\n",
    "    def _save_predictions(self, dataset, sample_size=1):\n",
    "\n",
    "        for id, (image, mask) in enumerate(dataset.take(sample_size), start=1):\n",
    "            pred_mask = self.model.predict(image)\n",
    "            pred_mask = self._create_mask(pred_mask)\n",
    "\n",
    "            image = image[0]\n",
    "            ground_truth_mask = mask[0]\n",
    "            self._save_image_and_masks(image, ground_truth_mask, pred_mask, image_id=id)\n",
    "\n",
    "    def evaluate(self, test_dataset, sample_size=5):\n",
    "\n",
    "        result = self.model.evaluate(test_dataset)\n",
    "        #print(f'Accuracy: {result[1] * 100:.2f}%')\n",
    "        mlflow.log_metric(\"Accuracy\", result[1] * 100)\n",
    "\n",
    "        self._save_predictions(test_dataset, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(AUTOTUNE, BUFFER_SIZE, BATCH_SIZE):\n",
    "\n",
    "    dataset, info = tfdata.load('oxford_iiit_pet', with_info=True)\n",
    "    #print(info)\n",
    "\n",
    "    TRAIN_SIZE = info.splits['train[:80%]'].num_examples\n",
    "    VALIDATION_SIZE = info.splits['test[:80%]'].num_examples\n",
    "\n",
    "    train_dataset = (dataset['train'].take(TRAIN_SIZE) #dataset['train']\n",
    "                        .map(load_image, num_parallel_calls=AUTOTUNE)\n",
    "                        .cache()\n",
    "                        .shuffle(BUFFER_SIZE)\n",
    "                        .batch(BATCH_SIZE)\n",
    "                        .repeat()\n",
    "                        .prefetch(buffer_size=AUTOTUNE))\n",
    "    \n",
    "    test_dataset = (dataset['test'].take(VALIDATION_SIZE) #dataset['test']\n",
    "                    .map(lambda d: load_image(d, train=False), num_parallel_calls=AUTOTUNE)\n",
    "                    .batch(BATCH_SIZE))\n",
    "    \n",
    "    return train_dataset, test_dataset, TRAIN_SIZE, VALIDATION_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataset, test_dataset, EPOCHS, STEPS_PER_EPOCH, VALIDATION_STEPS):\n",
    "\n",
    "    unet = UNet()\n",
    "    unet.train(train_dataset,\n",
    "                epochs=EPOCHS,\n",
    "                steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                validation_steps=VALIDATION_STEPS,\n",
    "                validation_dataset=test_dataset)\n",
    "    unet.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flow(task_runner=SequentialTaskRunner())\n",
    "def main():\n",
    "    \n",
    "    client = mlflow_setup()\n",
    "    experiment_name = 'IMAGE-SEGMENTATION'\n",
    "    create_mlflow_experiment(experiment_name)\n",
    "\n",
    "    BATCH_SIZE = 32\n",
    "    BUFFER_SIZE = 1000\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    train_dataset, test_dataset, TRAIN_SIZE, VALIDATION_SIZE = load_data(AUTOTUNE, BUFFER_SIZE, BATCH_SIZE)\n",
    "\n",
    "    EPOCHS = 50\n",
    "    STEPS_PER_EPOCH = TRAIN_SIZE // BATCH_SIZE\n",
    "    VALIDATION_SUBSPLITS = 5\n",
    "    VALIDATION_STEPS = VALIDATION_SIZE // BATCH_SIZE\n",
    "    VALIDATION_STEPS //= VALIDATION_SUBSPLITS\n",
    "    \n",
    "    train_dataset, test_dataset, TRAIN_SIZE, VALIDATION_SIZE = load_data(AUTOTUNE, BUFFER_SIZE, BATCH_SIZE)\n",
    "\n",
    "    with mlflow.start_run() as run:\n",
    "        \n",
    "        print(\"MLflow:\")\n",
    "        print(\"  run_id:\",run.info.run_id)\n",
    "        print(\"  experiment_id:\",run.info.experiment_id)\n",
    "\n",
    "        mlflow.set_tag(\"version.mlflow\", mlflow.__version__)\n",
    "        mlflow.set_tag(\"version.tensorflow\", tf.__version__)\n",
    "\n",
    "        mlflow.log_param(\"epochs\", EPOCHS)\n",
    "        mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
    "        mlflow.log_param(\"Train_size\", TRAIN_SIZE)\n",
    "        mlflow.log_param(\"Test_size\", VALIDATION_SIZE)\n",
    "\n",
    "        train_model(train_dataset, test_dataset, EPOCHS, STEPS_PER_EPOCH, VALIDATION_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:38:18.389 | INFO    | prefect.engine - Created flow run 'magenta-python' for flow 'main'\n",
      "13:38:18.390 | INFO    | Flow run 'magenta-python' - Using task runner 'SequentialTaskRunner'\n",
      "13:38:18.401 | WARNING | Flow run 'magenta-python' - No default storage is configured on the server. Results from this flow run will be stored in a temporary directory in its runtime environment.\n",
      "13:38:18.492 | INFO    | Flow run 'magenta-python' - Created task run 'create_mlflow_experiment-863ae521-7' for task 'create_mlflow_experiment'\n",
      "13:38:18.578 | INFO    | Task run 'create_mlflow_experiment-863ae521-7' - Finished in state Completed()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment exists\n",
      "Experiment_id: 4\n",
      "Name: IMAGE-SEGMENTATION\n",
      "Artifact Location: //home/pmspraju/tracking-server/mlruns/\n",
      "Tags: {'version': 'v1', 'priority': 'P1'}\n",
      "Lifecycle_stage: active\n",
      "Last Updated timestamp: 1679840590072\n",
      "MLflow:\n",
      "  run_id: c4b23c0aaa50484d94ebcf11dde3a37e\n",
      "  experiment_id: 4\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 963s 10s/step - loss: 0.7381 - accuracy: 0.6779 - val_loss: 0.8601 - val_accuracy: 0.6576\n",
      "Epoch 2/50\n",
      "92/92 [==============================] - 987s 11s/step - loss: 0.6051 - accuracy: 0.7470 - val_loss: 0.7581 - val_accuracy: 0.6931\n",
      "Epoch 3/50\n",
      "92/92 [==============================] - 1040s 11s/step - loss: 0.5525 - accuracy: 0.7700 - val_loss: 0.7750 - val_accuracy: 0.6944\n",
      "Epoch 4/50\n",
      "92/92 [==============================] - 1012s 11s/step - loss: 0.5031 - accuracy: 0.7906 - val_loss: 0.8039 - val_accuracy: 0.6919\n",
      "Epoch 5/50\n",
      "92/92 [==============================] - 968s 11s/step - loss: 0.4623 - accuracy: 0.8076 - val_loss: 1.0786 - val_accuracy: 0.6736\n",
      "Epoch 6/50\n",
      "92/92 [==============================] - 963s 10s/step - loss: 0.4150 - accuracy: 0.8276 - val_loss: 0.6064 - val_accuracy: 0.7728\n",
      "Epoch 7/50\n",
      "92/92 [==============================] - 958s 10s/step - loss: 0.3838 - accuracy: 0.8405 - val_loss: 0.6404 - val_accuracy: 0.7616\n",
      "Epoch 8/50\n",
      "92/92 [==============================] - 957s 10s/step - loss: 0.3514 - accuracy: 0.8538 - val_loss: 0.6836 - val_accuracy: 0.7699\n",
      "Epoch 9/50\n",
      "92/92 [==============================] - 961s 10s/step - loss: 0.3208 - accuracy: 0.8659 - val_loss: 0.7164 - val_accuracy: 0.7657\n",
      "Epoch 10/50\n",
      "92/92 [==============================] - 959s 10s/step - loss: 0.2977 - accuracy: 0.8748 - val_loss: 0.7161 - val_accuracy: 0.7749\n",
      "Epoch 11/50\n",
      "92/92 [==============================] - 962s 10s/step - loss: 0.2778 - accuracy: 0.8828 - val_loss: 0.8431 - val_accuracy: 0.7545\n",
      "Epoch 12/50\n",
      "92/92 [==============================] - 963s 10s/step - loss: 0.2662 - accuracy: 0.8874 - val_loss: 0.8192 - val_accuracy: 0.7560\n",
      "Epoch 13/50\n",
      "92/92 [==============================] - 963s 10s/step - loss: 0.2505 - accuracy: 0.8934 - val_loss: 0.6088 - val_accuracy: 0.8053\n",
      "Epoch 14/50\n",
      "92/92 [==============================] - 960s 10s/step - loss: 0.2352 - accuracy: 0.8991 - val_loss: 0.7544 - val_accuracy: 0.7889\n",
      "Epoch 15/50\n",
      "92/92 [==============================] - 966s 11s/step - loss: 0.2244 - accuracy: 0.9033 - val_loss: 0.8759 - val_accuracy: 0.7763\n",
      "Epoch 16/50\n",
      "92/92 [==============================] - 965s 10s/step - loss: 0.2105 - accuracy: 0.9087 - val_loss: 1.0439 - val_accuracy: 0.7585\n",
      "Epoch 17/50\n",
      "92/92 [==============================] - 963s 10s/step - loss: 0.2059 - accuracy: 0.9105 - val_loss: 1.2023 - val_accuracy: 0.7485\n",
      "Epoch 18/50\n",
      "92/92 [==============================] - 962s 10s/step - loss: 0.1962 - accuracy: 0.9142 - val_loss: 1.1169 - val_accuracy: 0.7611\n",
      "Epoch 19/50\n",
      "92/92 [==============================] - 969s 11s/step - loss: 0.1869 - accuracy: 0.9175 - val_loss: 1.0634 - val_accuracy: 0.7699\n",
      "Epoch 20/50\n",
      "92/92 [==============================] - 969s 11s/step - loss: 0.1832 - accuracy: 0.9192 - val_loss: 0.9348 - val_accuracy: 0.7856\n",
      "Epoch 21/50\n",
      "92/92 [==============================] - 976s 11s/step - loss: 0.1775 - accuracy: 0.9212 - val_loss: 0.9802 - val_accuracy: 0.7794\n",
      "Epoch 22/50\n",
      "92/92 [==============================] - 979s 11s/step - loss: 0.1716 - accuracy: 0.9236 - val_loss: 1.1299 - val_accuracy: 0.7738\n",
      "Epoch 23/50\n",
      "92/92 [==============================] - 971s 11s/step - loss: 0.1660 - accuracy: 0.9258 - val_loss: 1.3692 - val_accuracy: 0.7518\n",
      "Epoch 24/50\n",
      "92/92 [==============================] - 971s 11s/step - loss: 0.1581 - accuracy: 0.9288 - val_loss: 1.1439 - val_accuracy: 0.7692\n",
      "Epoch 25/50\n",
      "92/92 [==============================] - 975s 11s/step - loss: 0.1563 - accuracy: 0.9296 - val_loss: 1.3582 - val_accuracy: 0.7470\n",
      "Epoch 26/50\n",
      "92/92 [==============================] - 971s 11s/step - loss: 0.1512 - accuracy: 0.9316 - val_loss: 1.0938 - val_accuracy: 0.7796\n",
      "Epoch 27/50\n",
      "92/92 [==============================] - 969s 11s/step - loss: 0.1455 - accuracy: 0.9340 - val_loss: 1.4358 - val_accuracy: 0.7628\n",
      "Epoch 28/50\n",
      "92/92 [==============================] - 968s 11s/step - loss: 0.1422 - accuracy: 0.9352 - val_loss: 1.1552 - val_accuracy: 0.7819\n",
      "Epoch 29/50\n",
      "92/92 [==============================] - 973s 11s/step - loss: 0.1386 - accuracy: 0.9368 - val_loss: 1.2533 - val_accuracy: 0.7711\n",
      "Epoch 30/50\n",
      "92/92 [==============================] - 970s 11s/step - loss: 0.1339 - accuracy: 0.9386 - val_loss: 1.4516 - val_accuracy: 0.7649\n",
      "Epoch 31/50\n",
      "92/92 [==============================] - 976s 11s/step - loss: 0.1287 - accuracy: 0.9408 - val_loss: 1.3730 - val_accuracy: 0.7647\n",
      "Epoch 32/50\n",
      "92/92 [==============================] - 978s 11s/step - loss: 0.1279 - accuracy: 0.9412 - val_loss: 1.5947 - val_accuracy: 0.7496\n",
      "Epoch 33/50\n",
      "92/92 [==============================] - 978s 11s/step - loss: 0.1258 - accuracy: 0.9420 - val_loss: 1.3069 - val_accuracy: 0.7754\n",
      "Epoch 34/50\n",
      "92/92 [==============================] - 983s 11s/step - loss: 0.1233 - accuracy: 0.9430 - val_loss: 1.1378 - val_accuracy: 0.7938\n",
      "Epoch 35/50\n",
      "92/92 [==============================] - 986s 11s/step - loss: 0.1172 - accuracy: 0.9455 - val_loss: 1.3795 - val_accuracy: 0.7732\n",
      "Epoch 36/50\n",
      "92/92 [==============================] - 988s 11s/step - loss: 0.1153 - accuracy: 0.9463 - val_loss: 1.5538 - val_accuracy: 0.7647\n",
      "Epoch 37/50\n",
      "92/92 [==============================] - 895s 10s/step - loss: 0.1126 - accuracy: 0.9475 - val_loss: 1.3038 - val_accuracy: 0.7807\n",
      "Epoch 38/50\n",
      "92/92 [==============================] - 696s 8s/step - loss: 0.1109 - accuracy: 0.9482 - val_loss: 1.3147 - val_accuracy: 0.7878\n",
      "Epoch 39/50\n",
      "92/92 [==============================] - 698s 8s/step - loss: 0.1096 - accuracy: 0.9487 - val_loss: 1.2040 - val_accuracy: 0.8008\n",
      "Epoch 40/50\n",
      "92/92 [==============================] - 702s 8s/step - loss: 0.1061 - accuracy: 0.9501 - val_loss: 1.3995 - val_accuracy: 0.7859\n",
      "Epoch 41/50\n",
      "92/92 [==============================] - 711s 8s/step - loss: 0.1033 - accuracy: 0.9513 - val_loss: 1.4912 - val_accuracy: 0.7779\n",
      "Epoch 42/50\n",
      "92/92 [==============================] - 2565s 28s/step - loss: 0.1017 - accuracy: 0.9519 - val_loss: 1.5578 - val_accuracy: 0.7756\n",
      "Epoch 43/50\n",
      "92/92 [==============================] - 870s 9s/step - loss: 0.0987 - accuracy: 0.9531 - val_loss: 1.1779 - val_accuracy: 0.7984\n",
      "Epoch 44/50\n",
      "92/92 [==============================] - 882s 10s/step - loss: 0.0986 - accuracy: 0.9532 - val_loss: 1.2051 - val_accuracy: 0.7990\n",
      "Epoch 45/50\n",
      "92/92 [==============================] - 884s 10s/step - loss: 0.0960 - accuracy: 0.9542 - val_loss: 1.5093 - val_accuracy: 0.7808\n",
      "Epoch 46/50\n",
      "92/92 [==============================] - 890s 10s/step - loss: 0.0924 - accuracy: 0.9557 - val_loss: 1.2691 - val_accuracy: 0.7998\n",
      "Epoch 47/50\n",
      "92/92 [==============================] - 880s 10s/step - loss: 0.0911 - accuracy: 0.9562 - val_loss: 1.2932 - val_accuracy: 0.8032\n",
      "Epoch 48/50\n",
      "92/92 [==============================] - 882s 10s/step - loss: 0.0916 - accuracy: 0.9560 - val_loss: 1.4104 - val_accuracy: 0.7863\n",
      "Epoch 49/50\n",
      "92/92 [==============================] - 879s 10s/step - loss: 0.0888 - accuracy: 0.9571 - val_loss: 1.3615 - val_accuracy: 0.8001\n",
      "Epoch 50/50\n",
      "92/92 [==============================] - 883s 10s/step - loss: 0.0863 - accuracy: 0.9581 - val_loss: 1.4330 - val_accuracy: 0.7914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:04:12.347 | WARNING | absl - Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 17). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpbsoofv8w/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:04:14.570 | INFO    | tensorflow - Assets written to: /tmp/tmpbsoofv8w/model/data/model/assets\n",
      "/tmp/ipykernel_2086/2284606224.py:105: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-darkgrid')\n",
      "/tmp/ipykernel_2086/2284606224.py:105: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-darkgrid')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 300s 3s/step - loss: 1.4097 - accuracy: 0.7930\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:09:43.220 | INFO    | Flow run 'magenta-python' - Finished in state Completed('All states completed.')\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imageResolution-c2hlZ-SW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "744a34a68aed0be7f165d9fabc8c1bc8fbffaff0c14db28a8c988df6d9e326c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
